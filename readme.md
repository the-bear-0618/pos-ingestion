# POS Ingestion Pipeline

![Build Status](https://github.com/your-org/pos-ingestion/actions/workflows/test.yml/badge.svg)

This repository contains the complete data pipeline for ingesting Point of Sale (POS) data, validating it against formal data contracts, and loading it into Google BigQuery.

## ğŸ›ï¸ Architecture Overview

The system is designed as a resilient, event-driven pipeline composed of two main microservices and managed Google Cloud infrastructure.

* **`pos-poller`**: A Python service that polls the third-party POS OData API on a schedule, performs minimal transformation (PascalCase to snake_case), and publishes each record as a distinct event to a Pub/Sub topic.
* **`pos-processor`**: A Python service that subscribes to the Pub/Sub topic. It validates each incoming event against a corresponding JSON Schema, and upon successful validation, inserts the clean data into the appropriate BigQuery table. Failed messages are routed to a Dead-Letter Queue (DLQ).

*(This is a great place to embed the `Comprehensive System Architecture` diagram you created!)*
```
![Comprehensive System Architecture Diagram] (/docs/images/Editor _ Mermaid Chart-2025-06-30-184011.png)
![Comprehensive System Architecture Diagram] (docs/images/Editor _ Mermaid Chart-2025-06-30-220559.png)
```

### Project Structure
```
pos-ingestion/
â”œâ”€â”€ .github/              # GitHub Actions CI/CD Workflows
â”œâ”€â”€ pos-poller/           # The data polling service
â”œâ”€â”€ pos-processor/        # The schema validation and BigQuery insertion service
â”œâ”€â”€ schemas/              # The Single Source of Truth: JSON Schema data contracts
â”œâ”€â”€ terraform/            # Infrastructure as Code for all GCP resources
â”œâ”€â”€ .gitignore            # Files and directories to ignore in version control
â”œâ”€â”€ cloudbuild.yaml       # Secure container build definitions for GCP
â”œâ”€â”€ docker-compose.yml    # Local development and testing environment
â””â”€â”€ README.md             # This file
```

---

## ğŸš€ Getting Started

### Prerequisites

Ensure you have the following tools installed on your local machine:
* Git
* Docker & Docker Compose
* Google Cloud SDK (`gcloud`)
* Terraform

### Local Development Setup

This setup uses Docker Compose to run the entire pipeline locally, including a Pub/Sub emulator.

**1. Clone the Repository**
```bash
git clone [https://github.com/your-org/pos-ingestion.git](https://github.com/your-org/pos-ingestion.git)
cd pos-ingestion
```

**2. Configure Local Environment**
Create a `.env` file for local configuration. A template is provided.
```bash
cp .env.example .env
```
Now, **edit the `.env` file** with your local settings and POS API credentials.

**3. Run the Local Environment**
This single command will build the Docker images for the services and start the entire local pipeline.
```bash
docker-compose up --build
```
The services will be available:
* **Pub/Sub Emulator**: `localhost:8085`
* **pos-poller**: `localhost:8080`
* **pos-processor**: `localhost:8081`

**4. Set up Local Pub/Sub Topics**
The Pub/Sub emulator starts up empty. You need to create the topic, subscription, and DLQ. Run this script in a **new terminal window** after `docker-compose up` is running.

```bash
./setup_local_pubsub.sh
```

**5. Trigger a Data Sync**
You can now trigger the poller to fetch data by sending a POST request to its `/sync` endpoint.

```bash
curl -X POST -H "Content-Type: application/json" \
  -d '{
    "days_back": 1,
    "endpoints": ["Checks", "ItemSales"]
  }' \
  http://localhost:8080/sync
```
You should see logs from both the `pos-poller` and `pos-processor` in your `docker-compose` terminal window.

---

## ğŸ§ª Testing

This project uses `pytest`. To run all unit and integration tests for both services, execute the following command from the root directory:

```bash
pytest
```

---

## â˜ï¸ Infrastructure Deployment

All GCP resources (BigQuery, Pub/Sub, IAM, etc.) are managed via Terraform.

1.  **Initialize Terraform**
    ```bash
    cd terraform
    terraform init
    ```

2.  **Plan the Deployment**
    Create a `terraform.tfvars` file to specify your project ID.
    ```
    gcp_project_id = "your-gcp-project-id"
    ```
    Then run the plan to see what resources will be created.
    ```bash
    terraform plan -var-file="terraform.tfvars"
    ```

3.  **Apply the Configuration**
    ```bash
    terraform apply -var-file="terraform.tfvars"
    ```

---

## CI/CD

This repository is configured with GitHub Actions for Continuous Integration and Deployment.

* **On Pull Request:** The `test.yml` workflow is triggered to run all `pytest` tests.
* **On Merge to `main`:** The `deploy.yml` workflow is triggered. It orchestrates the process of building the service images using Google Cloud Build and deploying them to Cloud Run.


Overall File Structure as of 7.1.25 (Pre-PROD Push)

pos-ingestion/
â”œâ”€â”€ .github/                       # Directory for GitHub Actions workflows (CI/CD)
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ deploy.yml             # Deploys application to Cloud Run on push to main
â”‚       â””â”€â”€ test.yml               # Runs tests automatically on pull requests
â”œâ”€â”€ docs/                          # Project documentation and assets
â”‚   â””â”€â”€ images/
â”‚       â””â”€â”€ system-architecture.png # Visual diagram of the pipeline architecture
â”œâ”€â”€ pos-poller/                    # Service to poll the POS API and publish events
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â””â”€â”€ test_poller.py         # Unit and integration tests for the poller
â”‚   â”œâ”€â”€ config.py                  # Configuration for poller endpoints and fields
â”‚   â”œâ”€â”€ Dockerfile                 # Instructions to build the poller's container image
â”‚   â”œâ”€â”€ main.py                    # Web server entry point (Flask) for the poller
â”‚   â”œâ”€â”€ poller.py                  # Core logic for fetching and publishing data
â”‚   â”œâ”€â”€ requirements.txt           # Python libraries required by the poller
â”‚   â””â”€â”€ utils.py                   # Helper functions (e.g., snake_case conversion)
â”œâ”€â”€ pos-processor/                 # Service to consume events, validate, and load to BigQuery
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â””â”€â”€ test_processor.py      # Unit and integration tests for the processor
â”‚   â”œâ”€â”€ Dockerfile                 # Instructions to build the processor's container image
â”‚   â”œâ”€â”€ main.py                    # Web server entry point (Flask) that receives Pub/Sub messages
â”‚   â”œâ”€â”€ requirements.txt           # Python libraries required by the processor
â”‚   â””â”€â”€ schema_validator.py        # Core logic for loading schemas and validating data
â”œâ”€â”€ schemas/                       # Single Source of Truth: All JSON Schema data contracts
â”‚   â”œâ”€â”€ base_event.json           # Defines the common message envelope for all events
â”‚   â”œâ”€â”€ checks.json                # Schema for the 'Checks' endpoint
â”‚   â”œâ”€â”€ customers.json             # Schema for the 'Customers' endpoint
â”‚   â”œâ”€â”€ item_sale_adjustments.json # Schema for the 'ItemSaleAdjustments' endpoint
â”‚   â”œâ”€â”€ item_sale_components.json  # Schema for the 'ItemSaleComponents' endpoint
â”‚   â”œâ”€â”€ item_sale_taxes.json       # Schema for the 'ItemSaleTaxes' endpoint
â”‚   â”œâ”€â”€ item_sales.json            # Schema for the 'ItemSales' endpoint
â”‚   â”œâ”€â”€ paidouts.json              # Schema for the 'Paidouts' endpoint
â”‚   â”œâ”€â”€ payments.json              # Schema for the 'Payments' endpoint
â”‚   â””â”€â”€ time_records.json          # Schema for the 'TimeRecords' endpoint
â”œâ”€â”€ terraform/                     # Infrastructure as Code (IaC) for all GCP resources
â”‚   â”œâ”€â”€ backend.tf                 # Configures the GCS bucket for remote state storage
â”‚   â”œâ”€â”€ bigquery.tf                # Defines the BigQuery dataset and all 9 tables
â”‚   â”œâ”€â”€ iam.tf                     # Defines service accounts and their permissions
â”‚   â”œâ”€â”€ pubsub.tf                  # Defines Pub/Sub topics, subscriptions, and the DLQ
â”‚   â”œâ”€â”€ terraform.tfvars           # File to provide values for variables (e.g., project_id)
â”‚   â””â”€â”€ variables.tf               # Declares input variables for the Terraform configuration
â”œâ”€â”€ .gitignore                     # Specifies files and folders for Git to ignore
â”œâ”€â”€ capture_api_data.py            # Utility script for capturing raw API data during development
â”œâ”€â”€ cloudbuild.yaml                # Defines the build process for Google Cloud Build
â”œâ”€â”€ docker-compose.yml             # Orchestrates the entire local development environment
â”œâ”€â”€ README.md                      # Main documentation for the project
â””â”€â”€ setup_local_pubsub.sh          # Helper script to initialize the local Pub/Sub emulator

9 directories, 36 files